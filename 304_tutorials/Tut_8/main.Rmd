---
title: "POPLHLTH 304 tutorial 8: logistic regression"
author: "Simon Thornley and John Sluyter"
date: "`r format(Sys.time(), '%e %B, %Y')`"
output: 
  learnr::tutorial:
        progressive: true
        allow_skip: true
  # prettydoc::html_pretty:
  # theme: tactile
runtime: shiny_prerendered
description: >
  Conducting logistic regression.
editor_options: 
  markdown: 
    wrap: 72
---

<center>

![Marijuana joint](./images/marijuana_joint.avif "Joint"){width="50%"}

</center>

<br></br>


If you have a few minutes, this is worth watching on the subject of
["how marijuana
works"](https://www.youtube.com/watch?v=Kg8wrzmIdyI){target="_blank"},
by National Geographic. It is a useful introduction to what 
we'll be studying today.
<br></br>

This session, we will focus on modelling of *binary outcomes* in
epidemiology. These are very common, as many times we will be dealing
with hypotheses related to risk factors or causes of diseases which we 
either do or do not have.

We will learn how to:

-   Plot relationships with binary variables
-   Run and interpret regression models
-   Calculate attributable risk
-   Use regression nomograms to better understand regression models
-   Interpret the model fit

## Introduction to logistic regression

We have, by now, covered linear regression which is used when an outcome
variable is continuous. For count outcomes, we use Poisson regression
and its derivatives, where the outcome variable is a count or integer.
For outcomes that are binary ("yes" or "no" for example), logistic
regression is frequently the answer.

### Mathematical theory

Logistic models rely on a mathematical trick to transform the right hand
side of a regression equation
$\beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_z x_z$, with $z$
exposures or confounders (covariates) from a real number (positive or
negative, with decimals) to a probability (constrained to be between
zero and one).

This is accomplished by the magical **logit** transformation of the
outcome. The **logit** transformation or **log(odds)** is:

$\text{logit}(p) = ln\left( \frac{p}{1-p} \right)$

Where: $p$ is a binary variable where `1` indicates the presence and `0`
the absence of disease.

The logistic model is therefore:

$\text{logit}(p) = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_z x_z + \epsilon$

Notice, we've added the $\epsilon$ or error term here to describe the
distance between observed and model predicted data (residual).

### Interpreting $\beta$ coefficients

The main thing to remember with logistic models is that the $\beta$
coefficients need to be exponentiated to be easily interpretable in
epidemiological terms. They are interpreted as **odds ratios** which
describe the increased or decreased odds of the outcome $p$ for a one
unit change in the exposure or covariate $x$. This has parallels with
linear and count regressions.

Remember that odds are simply a transformation of risk:

$\text{odds} = \frac{p}{1 - p}$

Note also, that on a **logit** scale, the logistic regression function
is linear. This is nicely illustrated in the following [shiny
app](https://supsych.shinyapps.io/logistic_regression/){target="_blank"},
by ticking the **Plot in logit domain** check box.

Statistical tests of associations for $\beta$ coefficients are based on a value of `0` for the null
 hypothesis. Exponentiated, this corresponds to an odds ratio of `1`, which of course is a null value of the odds ratio.
 
## Getting started

We will be leaning on the following libraries to do these analyses:

-   [`regplot`](https://cran.r-project.org/web/packages/regplot/index.html){target="_blank"}
-   [`PerformanceAnalytics`](https://cran.r-project.org/web/packages/PerformanceAnalytics/index.html){target="_blank"}
-   [`jtools`](https://jtools.jacob-long.com/){target="_blank"}
-   [`corrplot`](https://r-coder.com/correlation-plot-r/){target="_blank"}
-   [`finalfit`](https://finalfit.org/){target="_blank"}
-   [`car`](https://socialsciences.mcmaster.ca/jfox/Books/Companion/index.html){target="_blank"}
-   [`arsenal`](https://mayoverse.github.io/arsenal/articles/tableby.html){target="_blank"}

## **Background**

<center>

![Marijuana by Matthew Brodeur from
unsplash.com](./images/matthew-brodeur-qcCPIhhdgTw-unsplash.jpg "marijuana"){width="200"}

</center>

Imagine that we have been assigned a task to consider risk factors for
marijuana use, and we have in mind some programmes to help reduce
harmful use of the drug, but we need to know where to target this
resource. How can we do that?

Well, we have a survey of marijuana use in a representative population,
so we will analyse the data to see what the major risk behaviours are.

This is available in the
[NHANES](https://cran.r-project.org/web/packages/NHANES/NHANES.pdf){target="_blank"}
US national health survey.

What sort of factors do you think are likely to increase the risk of
taking recreational marijuana?

## **Summary of analysis**

1.  **Load libraries** and **import data** (here the data is imported
    from an installed library).

2.  **Check data integrity** (we will skip through this)

3.  **Subset data**. It doesn't make sense to ask very young children
    about their marijuana use! In an epidemiological study, the
    denominator should consist of people at risk of the event of
    interest!

4.  **Plot associations** between various demographic and behavioural
    risk factors and marijuana use. Conduct crude analyses, using odds
    ratios. We'll be using histograms, boxplots (`lattice` package) and
    Euler plots (`eulerr` package). Check the bivariable associations
    and correlations between variables
    `PerformanceAnalytics::chart.Correlation()`). For this, we will need
    to convert character or factor variables (`"Yes"` or `"No"`, or
    `TRUE` or `FALSE`) to numeric variables (`1` or `0`).

5.  If writing a paper, you would normally make a '**Table 1**' in which
    the various risk factors are divided by the outcome with column
    percentages and appropriate statistical tests. We will skip through
    this, but include it for completeness. This gives us similar
    information as in the previous point, but limited only to
    associations between exposures, confounders and outcome, rather than
    also between exposures and confounders as well. We will use the
    `arsenal::tableby()` function.

6.  **Logistic regression**, use `finalfit()` and `regplot()`to finalise and
    visualise models. `jtools` is useful for making initial model output
    interpretable!

7.  **Check model fit** (`car::marginalModelPlots()`) and adjust the
    model if necessary. The Hosmer-Lemeshow test is also relevant for checking 
    the calibration between observed and predicted values.

8.  **Interpret the study results**. Which variables are significantly
    associated Could there be other explanations other than a causal
    explanation? Could it be: reverse causation, measurement error,
    social desirability bias?

### Further reading

Weist et al. [Journal of Paediatrics and Child Health 51 (2015)
670--673](https://onlinelibrary-wiley-com.ezproxy.auckland.ac.nz/doi/pdf/10.1111/jpc.12895){target="_blank"}.

```{r setup, include=FALSE}

# if(!require(rio)) install.packages("rio")
# if(!require(rmarkdown)) install.packages("rmarkdown")
# if(!require(learnr)) install.packages("learnr")
# if(!require(tufte)) install.packages("tufte")
# if(!require(visdat)) install.packages("visdat")
# if(!require(DiagrammeR)) install.packages("DiagrammeR")
# if(!require(r2symbols)) install.packages("r2symbols")
# if(!require(skimr)) install.packages("skimr")
# if(!require(prettydoc)) install.packages("prettydoc")
# if(!require(gradethis)) install.packages("gradethis")
# if(!require(remotes)) install.packages("remotes")
# remotes::install_github("rstudio/gradethis")

# if(!require(NHANES)) install.packages("NHANES")
# if(!require(AF)) install.packages("AF")
# if(!require(arsenal)) install.packages("arsenal")

library(learnr)
library(tufte)
library(rio)
library(visdat)
library(fs)
library(ggplot2)
library(DiagrammeR)
library(r2symbols) # cheatsheet here: https://r2symbols.obi.obianom.com/symbols/
library(learnr)
library(gradethis)
library(skimr)
library(epiDisplay)
library(lattice)
library(finalfit)
library(tidyverse)
library(NHANES)
library(knitr)
library(kableExtra)
library(readr)
library(dplyr)
library(jtools)
library(ggstance)
library(broom.mixed)
library(car)
library(eulerr)
library(glmtoolbox) # Hosmer-Lemeshow test
#library(regplot)
library(AF)
library(PerformanceAnalytics)
library(arsenal)

gradethis::gradethis_setup()


# Load data
data("NHANES")

# Setup work space
options(width = 600)

# Restrict to adults
Na <- NHANES |> subset(Age >= 20) |> data.frame()

# Restrict to those with complete responses to Marijuana variable
M <- Na |> subset(!is.na(Na$Marijuana))

M$SmokeCurrent <- ifelse(M$SmokeNow == "Yes", "Yes", "No")

M$All <- 1 # Assigns `1` to all rows. We’ll use this to create an ellipse for all participants
M$Marijuana_factor <- M$Marijuana |> factor() |> relevel(ref = "Yes")
M$HardDrugs_factor <- M$HardDrugs |> factor() |> relevel(ref = "Yes")
M$Alcohol_factor <- M$Alcohol12PlusYr |> factor() |> relevel(ref = "Yes")

# Euler plot
fit1 <- eulerr::euler(M[,c("Marijuana_factor",
                           "HardDrugs_factor",
                           "Alcohol_factor"  ,"All")] |> na.omit(),
                      shape = "ellipse")

# Logistic model
model <- glm(Marijuana ~ HardDrugs, data = M,
             family = binomial(link = "logit"))

M$HardDrugs_bin <- ifelse(M$HardDrugs == "Yes", 1, 0)
M$SmokeNow_bin <- ifelse(M$SmokeNow == "Yes", 1, 0)
M$Male <- ifelse(M$Gender == "male", 1, 0)
M$Marijuana_bin <- ifelse(M$Marijuana == "Yes", 1, 0)
M$Alcohol_bin <- ifelse(M$Alcohol12PlusYr == "Yes", 1, 0)

model_bin <- glm(Marijuana_bin ~ SmokeNow_bin + HardDrugs_bin + Age + Race3+
               Male + Alcohol_bin, data = M,
             family = binomial(link = "logit"))

# regplot(model)
# car::marginalModelPlots(model, sd = TRUE)

mycounter <- 0
Qnum <- function(count = mycounter){
  mycounter <<- mycounter + 1
  paste0("Question ", mycounter)
}

# tutorial_options(
#   exercise.timelimit = 60,
#   # A simple checker function that just returns the message in the check chunk
#   exercise.checker = function(check_code, ...) {
#     list(
#       message = eval(parse(text = check_code, ...)),
#       correct = logical(0),
#       type = "info",
#       location = "append"
#     )
#   }
# )
knitr::opts_chunk$set(error = TRUE)
```

## **Loading libraries and data**

Again, we use the `pacman` library to load other libraries.

```{r library_1, eval = FALSE}
if(!require(pacman)) install.packages("pacman")


pacman::p_load(magrittr, epiDisplay, tidyverse,
               rio, NHANES, visdat, skimr, finalfit, knitr, 
               kableExtra, readr, dplyr, lattice, jtools, 
               ggstance, broom.mixed, car, eulerr, regplot, 
               AF,# Model based attributable fractions 
               PerformanceAnalytics, # correlations    
              arsenal) # Making tables

## Respond with a "Y" if asked for a personal library.
update.packages(ask = FALSE)
```

Run the next line of code to increase the width of the output in the
console.

```{r data_1, exercise = TRUE}
## Setup work space
options(width = 600)
```

Load our data, which is the NHANES dataset.

```{r data_2, exercise = TRUE}
## Load data----
data("NHANES")
```

Use `?` to find out more about this dataset.

```{r data_3, exercise = TRUE}
?NHANES
```

It doesn't really make sense to ask children about marijuana use. So
we'll focus on those aged 20 or over. Run this line of code, which
creates a new dataset called `Na` that includes only those aged ≥20
years.

```{r data_4, exercise = TRUE}
## Restrict to adults
Na <- NHANES |> subset(Age >= 20) |> data.frame()
```

## **Univariable distributions and associations**

In this section, we'll look at the distribution of age and behaviour
group, and see what groups are associated with marijuana use.\
Use the `epiDisplay::tab1()` function to view the groups sizes for our outcome,
`Marijuana`.

```{r univariable_1, exercise = TRUE}
Na$Marijuana |> tab1()
```

[Notice that there are a lot of missing responses
(`NAs`)?]{style="color:blue"}\
Remove these with the next line of code which creates a new `data.frame`
called `M`:\
- `is.na()` selects `NA` (missing) values and `!` means "not"\
- So, `!is.na()` findings selects non-missing values.

Restrict to those with complete responses to `Marijuana` variable

```{r univaiable_2, exercise = TRUE}
M <- Na |> subset(!is.na(Na$Marijuana))
```

Use the `skim()` function to get a summary of our dataset

```{r univariable_3, exercise = TRUE}
M |> skim()
```

Check for any implausible values (check `p0` and
`p100`).

#### Outcome: **marijuana use**

We just removed missing responses for `Marijuana.` Check if they've been
removed.

```{r univariable_4, exercise = TRUE}
M$Marijuana |> tab1()
```

#### Do depressed people self-medicate with marijuana?

Inspect the groups for the `Depressed` variable.

```{r univariable_5, exercise = TRUE}
M$Depressed |> tab1()
```

You should get three depression groups: **none**, **several** and **most**, indicating the number of days in the 
last month that the subject felt *"down"* or *"depressed"* or *"hopeless"*.\

Use the `str()` function to find out what type of variable our outcome is.
For our analyses, it should be of the type, `factor`. Check that.

```{r univariable_6, exercise = TRUE}
M$Marijuana |> str()
```

[What type of variable is it?]{style="color:blue"}

The `cc()` function (from the `epiDisplay` package) calculates and
displays odds ratios.\
Use this to look at the relationship between `Depressed` (exposure) and
`Marijuana` (outcome).

```{r univariable_7, exercise = TRUE}
cc(M$Marijuana, M$Depressed) # Outcome first, exposure second
```

The *P*-values that we've previously dealt with in this course compare 2
groups. This *P*-value is different because we have 2 groups for
marijuana and 3 groups for depression -- but just one *P*-value. So the
*P*-value is *not* for comparing 2 groups. Instead, what it's testing
has a special name called the **main** effect or **overall** effect:
this is for the *overall association* between the two variables. As it's
almost zero, it means that, *overall*, depression is associated with
marijuana use.


##### **`r Qnum()`**

```{r univariable_8, echo = FALSE}
question("Look at the computer output and plot above. Which one of the following statements are true?",
answer("The odds of marijuana use is 1.22 units higher in the `Most` depressed group than in the `None` group", message = "No. remember we have calculated an odds ratio here."),
  answer("The odds of marijuana use is 1.22 *times* or 22% higher in the `Most` depressed than in the `None` group", message = "Yes, the word *times* suggests a ratio. The `None` group is the reference group as it has an odds ratio of `1`",  correct = TRUE),
  allow_retry = TRUE
)
```

#### Do men or women tend to take marijuana?

##### **`r Qnum()`**

Use the `epiDisplay::cc()` function to show the relationship between gender (exposure)
and marijuana use (outcome).

```{r univariable_9, exercise = TRUE}
# cc(dataset$outcome, dataset$exposure)
```

```{r univariable_9-solution}
cc(M$Marijuana, M$Gender)
```

::: {#univariable_9-hint}
**Hint:** What are `dataset`, `outcome` and `exposure` for this
question? Use the `names` function to list the variable names
:::

```{r univariable_9-check}
grade_this_code("Good! Note that, for the `cc()` function, we list the outcome first and exposure second ")
```

The graph shows that `female` is the reference group as the red arrow
points *away* from it.\

#### Is age associated with marijuana?

As age is a *continuous* variable, we'll use a box and whisker plot to
visualise its association with our outcome. For this, we'll use the
`lattice::bwplot` function, which we've used in a previous tutorial.

```{r univariable_10, exercise = TRUE}
lattice::bwplot(Age ~ Marijuana, data = M)
```

##### **`r Qnum()`**

```{r age_mari_ass, echo = FALSE}
question("From the plot above, is age strongly associated with marijuana use?",
answer("No, the distributions and median (central dot) is about the same.", message = "Yes. Although we haven't seen the result of the statistical test yet, there is little visual difference in the two distributions.", correct = TRUE),
  answer("Yes, marijuana users are on average much younger than non-users", message = "Although this might be what we expect, we don't see this in the plot, in fact the median is slightly older in marijuana users."),
  allow_retry = TRUE
)
```

#### Is BMI associated with marijuana?

##### **`r Qnum()`**

Create a plot with `lattice::bwplot()` to show the relationship between
`BMI` (exposure) and `Marijuana` use (outcome).

```{r univariable_11, exercise = TRUE}
# lattice::bwplot(variable1 ~ variable2, data = dataset)
```

```{r univariable_11-solution}
lattice::bwplot(BMI ~ Marijuana, data = M)
```

::: {#univariable_11-hint}
**Hint:** What are `variable1`, `variable2` and `dataset` for this
question? Use `?lattice::bwplot` if needed
:::

```{r univariable_11-check}
grade_this_code("Good job! Note that, for the `lattice::bwplot()` function, we list the continuous variable (`BMI`) first and categorical variable (`Marijuana`) second")
```

##### **`r Qnum()`**

```{r bmi_mari_ass, echo = FALSE}
question("From the plot above, is BMI strongly associated with marijuana use?",
answer("No, the distributions and median (central dot) is about the same.", message = "Yes. Although there is a longer tail in the 'no marijuana' group, the median values of BMI are very similar.", correct = TRUE),
  answer("Yes, marijuana use is strongly associated with BMI.", message = "Although this might be what we expect, we don't see this in the plot."),
  allow_retry = TRUE
)
```

#### Are smokers more likely to take marijuana?

The question for `SmokeNow` is only asked if a person smokes and it
tells us whether a smoker is an ex-smoker or a current-smoker. As
missing values indicate that the person doesn't smoke, we need to
replace these with a group called `Never` which indicates that the
person is a non-smoker (never smoked). To do this, run this code.

```{r univariable_12, exercise = TRUE}
M$SmokeCurrent <- ifelse(M$SmokeNow == "Yes", "Yes", "No")

## Visualise result
M$SmokeCurrent |> tab1()
```

Now use the `cc()` function to test whether `SmokeCurrent` is associated
with `Marijuana`.

```{r univariable_13, exercise = TRUE}
cc(M$Marijuana, M$SmokeCurrent)
```

[What is the reference group? How can we tell from our
results?]{style="color:blue"}

##### **`r Qnum()`**

```{r univariable_14, echo = FALSE}
question("Your output should read `P value = 0`. What's the most appropriate P-value to report?",
answer("P = 0", message = "No. A P-value is *never exactly 0.* There is always a chance (no matter how small), that our observed results (or one more extreme) could have occurred under the null hypothesis"),
  answer("P = 0.0001", message = "No, you should get a very high Chi-squared statistic (of 137), which indicates a P-value lower than this"),
  answer("P < 0.0001", message = "Yes, this is consistent with the very high Chi-squared statistic (of 137). It is a convention to report very low P-values this way",  correct = TRUE),
  allow_retry = TRUE
)
```

#### What about alcohol?

There are two alcohol variables and the first one we'll look at is
`AlcoholYear`: the number of days over the past year that the
participant drank alcohol. This is a continuous variable - so use a box
plot to visualise its relationship with the categorical outcome
`Marijuana` use.

```{r univariable_15, exercise = TRUE}
lattice::bwplot(AlcoholYear ~ Marijuana, data = M,
                ylab = "Average number of days in year alcohol consumed")
```

The second alcohol variable we'll consider is `AlcoholDay`: the average
number of drinks consumed on days that a participant drank alcohol. This
is also continuous variable - so use a box plot. 

Notice that individuals who responded to `AlcoholYear` with a `0` have a missing (`NA`) value for 
`AlcoholDay`. So we will have to correct `AlcoholDay` with the code below.

```{r univariable_16, exercise = TRUE}

## These values are missing but should be rather zero
M[M$AlcoholYear == 0,]$AlcoholDay |> summary()

## Change them to zero rather than missing
M$AlcoholDay_corrected <- ifelse(M$AlcoholYear == 0, 0, M$AlcoholDay)

M$AlcoholDay |> summary()

lattice::bwplot(AlcoholDay_corrected ~ Marijuana, data = M,
                ylab = "Average number of alcoholic drinks consumed 
                in a drinking session")
```


##### **`r Qnum()`**

```{r univariable_15_1, echo = FALSE}
question("Describe the distribution of the variable `AlcoholDay_corrected`.",
answer("Right skewed", message = "Correct!", correct = TRUE),
  answer("Symmetric or normally distributed", message = "No, have a close look at the box and whiskers, they are not symmetric!"),
  answer("Left skewed", message = "No, the outliers (blue dots) would be below the box in that case."),
  allow_retry = TRUE
)
```

With the **right-skewed** nature of the plot, we can use a log transformation 
to achieve a more symmetric distribution and better compare the two distributions.

We can confirm the need for a log transformation with the following code, 
which tests for the optimum power transformation $\lambda$ to achieve normality for the distribution.

A $P$-value that is significant means we reject the proposed value of $\lambda$, which in each case is 0 (meaning a log transformation is necessary) and 1 (meaning no transformation is necessary).

Notice that for log transformations, we often need to add a small number to avoid negative values.

```{r p_transform, exercise = TRUE}
car::powerTransform(I(AlcoholDay_corrected + 0.5) ~ 1, data = M,
                          family = "bcPower") |> summary()

car::symbox(~ I(AlcoholDay_corrected + 0.5), data = M)

```

The results indicate that both no transformation and a log transformation can be rejected and that the suggested transformation is `-0.30`, which is close to $\frac{1}{\sqrt[3]{x}}$. We could use such a transformation, but it is unfamiliar and hard to interpret the result.

However, the boxplot of a range of powers (`car::symbox()` function) shows that a close approximation to a symmetrical distribution is achieved with a log transformation. Since this is familiar and easy to interpret, we will use this.


```{r univariable_16_2, exercise = TRUE}
summary(M$AlcoholDay)

lattice::bwplot(I(log(AlcoholDay_corrected + 0.5)) ~ Marijuana, data = M,
                ylab = "ln(Average number of alcoholic drinks consumed 
                in a drinking session)")
```



[{style="color:blue"}

##### **`r Qnum()`**

```{r strength_alcohol, echo = FALSE}
question("Is the strength of the association between `AlcoholDay_corrected` and
`Marijuana` use weak, medium or strong?",
answer("medium", message = "Correct! The median values are the same, but the extreme high values are higher in marijuana users.", correct = TRUE),
  answer("Weak", message = "No, please try again."),
  answer("Strong", message = "No, please try again."),
  allow_retry = TRUE
)
```



#### Is marijuana a rich or poor issue?

The next risk factor is household income (`HHIncomeMid`). Another
continuous variable - so use a box plot.

```{r univariable_17, exercise = TRUE}
lattice::bwplot(HHIncomeMid ~ Marijuana, data = M,
                ylab = "Household income (US$ per year)")
```

[Does medium income differ much across marijuana
groups?]{style="color:blue"}

##### **`r Qnum()`**

```{r strength_hh_income, echo = FALSE}
question("Are marijuana users on average poorer or wealthier than abstainers?",

  answer("No difference", message = "No, please try again."),
  answer("Wealthier", message = "Correct! You can see the box of marijuana users is shifted higher", correct = TRUE),
  answer("Poorer", message = "No, please try again."),
  allow_retry = TRUE
)
```


#### Hard drugs?

`HardDrugs` is a categorical variable which indicates whether a
participant has ever tried cocaine, crack cocaine, heroin or
methamphetamine. Use the `cc()` function to test whether this behaviour
is associated with `Marijuana`.

```{r univariable_18, exercise = TRUE}
cc(M$Marijuana, M$HardDrugs)
```

[How would you describe the relationship between `HardDrugs` and
`Marijuana`?]{style="color:blue"}

## **Euler plots**

Euler plots are Venn diagrams that allow us to visualise associations
between categorical variables, such as exposures and outcomes. These
show the overlap between groups, which are represented as circles or
ellipses.

When interpreting a Euler plot please note that:\
- The **area** of the ellipse is proportional to the counts of
individuals with that characteristic.\
- More **overlap** between variables indicates *stronger* statistical
association between them.

Create a Euler plot to visualise associations between `Marijuana`,
`HardDrugs` and `Alcohol12PlusYr`.

```{r euler_1, exercise = TRUE}
M$All <- 1 # Assigns `1` to all rows. We’ll use this to create an ellipse for all participants

# Euler chooses the reference category as that to plot.
# We want to show the "Yes" category, so we'll change the reference
# from "No" to "Yes"

M$Marijuana_factor <- M$Marijuana |> factor() |> relevel(ref = "Yes")
M$HardDrugs_factor <- M$HardDrugs |> factor() |> relevel(ref = "Yes")
M$Alcohol_factor <- M$Alcohol12PlusYr |> factor() |> relevel(ref = "Yes")

# Euler plot
fit1 <- eulerr::euler(M[, c("Marijuana_factor",
                           "HardDrugs_factor",
                           "Alcohol_factor"  ,
                           "All")] |> na.omit(),# subset data and remove missing
                            shape = "ellipse")

plot(fit1, 
     main = "Marijuana, Alcohol and Hard drugs")
```

Note:\
- 3 variables in the plot end in `_Yes.` This is because the
`relevel(ref = "Yes")` in our code above makes `_Yes` the reference
group and the group that a Euler plot shows is the *reference* group\
- The area of the `Alcohol_factor_Yes` (dark pink) group is similar to
the area of the `All` (white) group. This indicates that a *high %* of
the sample were in the `Alcohol_factor=Yes` group.


```{r euler_2, exercise = TRUE}
M$Marijuana |> tab1()
M$HardDrugs |> tab1()
M$Alcohol12PlusYr |> tab1()
```

Let's add a legend to make it easier to see what the colours represent.
Also, to help quantify the degree of overlap between the groups, we'll
add count and % values.

```{r euler_3, exercise = TRUE}
plot(fit1, 
     legend = list(labels = c( "All", "Marijuana","HardDrugs", "Alcohol")),
     quantities = list(type = c("percent", "counts")))
```

Let's now focus on the association between `Alcohol_factor` and
`Marijuana_factor` by dropping `HardDrugs_factor` from our plot.

```{r euler_4, exercise = TRUE}
fit1 <- eulerr::euler(M[,c("Marijuana_factor",
                           "Alcohol_factor",
                           "All")] |>  na.omit(),
                          shape = "ellipse")

plot(fit1, main = "Marijuana and Alcohol")
```

## **Correlations between variables**

In the previous section, we created binary variables. This allows us to
look at correlations between them and other variables.

```{r correl_1, exercise = TRUE, warning = FALSE}
# Select variables for corelation analysis
my_data <- M[, c("Male",
                  "Marijuana_bin", "Alcohol_bin", "SmokeNow_bin",
                  "HardDrugs_bin", "Age")]

PerformanceAnalytics::chart.Correlation(my_data, 
                                        histogram=TRUE, pch=19)
```


```{r correlation, echo = FALSE}
question("Which variable is most strongly correlated with `Marijuana` use?",
answer("`HardDrugs`", message = "Yes, the correlation coefficient is greatest!", correct = TRUE),
  answer("`Alcohol_bin`", message = "Sorry, this is second strongest, please try again!"),
  answer("`Male`", message = "Sorry, this is third strongest, please try again!"),
  answer("`Age`", message = "Sorry, this is not associated, please try again!"),
  allow_retry = TRUE
)
```


## **Publication-quality by-group frequency tables (Table 1)**

We can use the `tableby()` function in the `arsenal` library to produce
a publication-quality table of count and % values for variables by
Marijuana use. Usually, this is `Table 1` in a publication.

```{r pub_table1_1, exercise = TRUE}
tab1 <- arsenal::tableby(Marijuana ~ # column
                            SmokeNow + # rows
                            HardDrugs + 
                            Race1 + 
                            Age + 
                            Gender + 
                            Depressed + 
                            Alcohol12PlusYr, 
                            data = M)

summary(tab1, text = TRUE)
```

In **posit**, this table can be copied to a clipboard (accessible from
the `File` tab) and pasted into a document using the code below. (Below,
we've named the file `trash`). This means you don't have to create a
table manually by typing in values -- so it saves you time!\
`write2html(tab1, "./trash.html", title = "My table in html")`\
`write2word(tab1, "./trash.doc", title = "My table in Word")`


```{r table1, echo = FALSE}
question("From the table above, which variables are associated with
`Marijuana`?",

  answer("None", message = "No, please try again."),
  answer("`HardDrugs`, `Race1`, `Gender`, `Depressed` and `Alcohol12PlusYr`", message = "Correct! The P-values for these comparisons are less than 0.05", correct = TRUE),
  answer("`HardDrugs` and `Race1`", message = "No, you've missed a few more, please try again!"),
  allow_retry = TRUE
)
```

## **Logistic model**

We'll build models using logistic regression. Before we start, our
categorical variables should be of the type, `factor` rather than
`character` - check if they are using the `str()` function.

```{r logistic_1, exercise = TRUE}
M$HardDrugs |> str()
M$SmokeNow |> str()
M$Race1 |> str()
```

Run this code to look at the association between `HardDrugs` and
`Marijuana`.\
- `glm()` stands for *generalised linear model*\
- `family = binomial(link = "logit")` indicates that we're using
logistic regression (logit link and binomial errors).

The basic logistic model is produced with the following code:

```{r logistic_2, exercise = TRUE}
model <- glm(Marijuana ~ HardDrugs, data = M,
             family = binomial(link = "logit"))
summary(model)

```

##### **`r Qnum()`**

```{r logistic_3, echo = FALSE}
question("For `HardDrugs`, you should get an `Estimate` value of 3.31305. What does this tell us?",
answer("The odds of marijuana use is 3.31305 higher among hard drug users", message = "No. *3.31305* is measured on a natural-log scale. So you need to exponentiate it to give a meaningful interpretation"),
  answer("The odds of marijuana use is 3.31305 *times* higher among hard drug users", message = "No. *3.31305* is measured on a natural-log scale. So you need to exponentiate it to give a meaningful interpretation"),
  answer("The odds of marijuana use is exp(3.31305) higher among hard drug users", message = "No. Exponentiating this converts it into an odds ratio. However, *exp(3.31305) higher* does not imply a ratio. What word do we need to add to imply a ratio?"),
  answer("The odds of marijuana use is exp(3.31305) *times* higher among hard drug users", message = "Yes, exponentiating 3.31305 converts it into an odds ratio. The word *times* suggests a ratio",  correct = TRUE),
  allow_retry = TRUE
)
```

Exponentiate the model $\beta$ coefficients with the `jtools::summ()`
function to derive odds ratios.

```{r logistic_4, exercise = TRUE}
jtools::summ(model, exp = TRUE)
```

##### **`r Qnum()`**

```{r logistic_5, echo = FALSE}
question("For `HardDrugsYes`, you should get an `exp(Est.)` value of 27.47. What does this tell us?",
answer("The odds of marijuana use is 27.47 higher among hard drug users", message = "No. 27.47 is a ratio. What word do we need to add to imply a ratio?"),
  answer("The odds of marijuana use is 27.47 *times* higher among hard drug users", message = "Well done! *27.47 times* implies a ratio and 27.47 is the odds ratio",  correct = TRUE),
  answer("The odds of marijuana use is exp(27.47) higher among hard drug users", message = "No. 27.47 is already an exponentiated value (R code above reads `exp = TRUE`) – you don’t need to exponentiate again!"),
  answer("The odds of marijuana use is exp(27.47) *times* higher among hard drug users", message = "No. 27.47 is already an exponentiated value (R code above reads `exp = TRUE`) – you don’t need to exponentiate again!"),
  allow_retry = TRUE
)
```

That was a *univariable* model with just one exposure (`HardDrugs`). Now
we'll create a *multivariable* model so that we can also adjust for
potential confounders.

```{r logistic_6, exercise = TRUE}
model <- glm(Marijuana ~ SmokeNow + HardDrugs + Race1 + Age + 
               Gender + Depressed + Alcohol12PlusYr, data = M,
             family = binomial(link = "logit"))

jtools::summ(model, exp = TRUE) # Exponentiate the beta coefficients to get odds ratios
```




## **Regression nomogram**

This is a visual summary of the contributions of predictors to an
outcome. We covered regression nomograms in the previous tutorial.

The following code draws a regression nomogram for our model from the
**logistic model** section.

`regplot(model)`

The output is shown below

![**Nomogram**](images/regplot.jpeg)

The nomogram depicts the structure of the model as a weighted average.

Each variable contributes a score `β(X-m) terms` based on the horizontal
deviation. These are summed for each individual to give a
`Total score`.\
Reading vertically down from the `Total score`, then gives a probability
of `Marijuana` use for each individual in the model (ranging between 0
and 1).

The horizontal distance between boxes or degree of spread of density
plots for variables represents the relative influence of the variable on
the model outcome.

The size of each box shows the distribution of categorical variables and
the density plot shows the distribution of continuous variables.

##### **`r Qnum()`**

```{r nomogram_2, echo = FALSE}
question("Look at the nomogram. Which of the following statements are true (select all that apply)?",
answer("`HardDrugs_bin` contributes more to the outcome (marijuana use) than `SmokeNow_bin`", message = "Statement 1 is true as `HardDrugs_bin` has a much longer scale",  correct = TRUE),
  answer("There are similar numbers of males and females", message = "Statement 2 is true also: for the `Male` density plot or histogram, the heights at 0 (females) and 1 (males) are similar, indicating similar numbers",  correct = TRUE),
  allow_retry = TRUE
)
```

## **Model fit**

How well a model fits the data can be assessed with a marginal model
plot, which shows the relationship between model predicted and observed
data.

```{r mmp, exercise = TRUE, warning = FALSE, error=FALSE}

# Note, all predictor variables must be factors for marginal model plot to work....
model <- glm(Marijuana ~ factor(SmokeNow) + factor(HardDrugs) + factor(Race1) + Age + 
               factor(Gender) + factor(Depressed) + factor(Alcohol12PlusYr), data = M,
             family = binomial(link = "logit"))

assign("M", M, envir = .GlobalEnv)
assign("model", model, envir = .GlobalEnv)

## Hosmer-Lemeshow test of model fit

glmtoolbox::hltest(model)

## marginal model plot
car::marginalModelPlots(model)
```

### Hosmer-Lemeshow test

This
[test](https://en.wikipedia.org/wiki/Hosmer%E2%80%93Lemeshow_test){target="_blank"}
divides the data into deciles based on ordered predicted probability of
the outcome (here: marijuana use). Then, the test then compares the
number of cases expected in each outcome group for expected and observed
counts. The null hypothesis is that there is no difference between
observed and model predicted cases throughout the range of risk.

You can see that the observed and expected counts in the output above
are very closely aligned.

Here, the $P$-value is 0.8105, which is not significant. Therefore, the
*calibration* of the model between observed and predicted numbers is
good. There are some limitations of the Hosmer-Lemeshow test discussed
[here](https://en.wikipedia.org/wiki/Hosmer%E2%80%93Lemeshow_test){target="_blank"}.

### Model fit - marginal model plot

We covered the theory related to marginal model plots in past
tutorials.\
Look at the plots above.

```{r mms, echo = FALSE}
question("From the marginal model plot, how well does the model fit the observed data?",
answer("Very well", message = "Well done, the lines almost completely overlap, indicating the moving averages of the data and model predictions are almostly completely aligned.",  correct = TRUE),
  answer("Poorly", message = "Sorry, try again. The overlapping lines suggest good model fit!"),
  allow_retry = TRUE
)
```

## **Publication-quality regression tables (Table 2)**

The code below allows us to produce publication-quality regression
tables. Usually, a regression table is `Table 2` in a publication. Note
that the `dependent` variable must be a `factor` variable with two
levels for this code to produce a *logistic regression* model, rather
than a linear or other regression.

```{r pub_table2_1, exercise = TRUE}
dependent <- "Marijuana"
explanatory <- c(
  "SmokeNow", 
  "HardDrugs","Race1", "Age", "Depressed", "Alcohol12PlusYr")

M |>
 finalfit(dependent, explanatory, metrics = FALSE) -> regression_table
  
  
regress_tab <- knitr::kable(regression_table, row.names = FALSE,
                            
                            align = c("l", "l", "r", "r", "r", "r"))

regress_tab |> kableExtra::kable_styling()


M |>
 finalfit(dependent, explanatory, metrics = TRUE) -> metrics_table

## show metrics only for display
metrics_table[[2]]

```

##### **`r Qnum()`**

```{r ethnic_group, echo = FALSE}
question("From the output above, which ethnic group (`Race1`) has the highest prevalence of marijuana use, before adjusting for other variables?",
answer("Black", message = "No, this is the reference group, and have a higher prevalence than Other, Hispanic and Mexican, but lower than 'White'."),
  answer("Hispanic", message = "Sorry, try again. This group has the lowest prevalence of marijuana use!"),
  answer("White", message = "Well done, the crude odds ratio is highest (1.41), indicating the highest (crude) prevalence of use of marijuana.", correct = TRUE),
  allow_retry = TRUE
)
```

## **Summary**

Today, we have extended regression to help us estimate the strength of 
association between risk factors and binary outcomes which
are often used in epidemiology. Logistic regression is used to predict
the probability of a binary outcome such as disease status.

-   Logistic regression is often used when the outcome is a binary
    variable and there are confounders to adjust for.
-   The principles are similar to usual regression, however the beta
    coefficients are exponentiated and interpreted as odds ratios
    (either crude or adjusted).
-   The outcome variable must have only two levels (`1` and `0`, `Yes`
    or `No`, or `TRUE` or `FALSE`).
-   We often present results in a table with both crude and adjusted
    odds ratios to see the effect of adjusting for confounders on an
    association.

## **Homework**

Repeat the above analyses looking at risk factors for alcohol use. For
the outcome, use the response to the question: consumption of at least
12 drinks of any type of alcoholic beverage in any one year
(`Alcohol12PlusYr == "Yes"`).

Please complete this in **posit**.
