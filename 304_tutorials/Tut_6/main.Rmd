---
title: "POPLHLTH 304 tutorial 6: count regression"
author: "Simon Thornley and John Sluyter"
date: "`r format(Sys.time(), '%e %B, %Y')`"
output: 
  learnr::tutorial:
        progressive: true
        allow_skip: true
  # prettydoc::html_pretty:
  #  theme: tactile
runtime: shiny_prerendered
description: >
  Inspecting our data for errors!
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
library(learnr)
library(tufte)
library(rio)
library(visdat)
library(fs)
library(ggplot2)
library(DiagrammeR)
library(r2symbols) # cheatsheet here: https://r2symbols.obi.obianom.com/symbols/
library(learnr)
library(gradethis)
library(skimr)
library(lattice)
library(epiDisplay)
library(finalfit)
library(vembedr)
library(png)

gradethis::gradethis_setup()
#setwd("./data")
# load("./Cloud/data/ARF_permethrin.rdata")
load("./data/ARF_permethrin.rdata")
# library(readxl)
# df <- read_xlsx("C:\\Users\\jslu002\\Desktop\\for_shiny.xlsx")
df <- data.frame(for_shiny)
df <- df[!duplicated(df),]

names(df) <- c("arf_count", "population", "Pacific_proportion",
               "Maori_proportion", "Permethrin_rate", "Suburb",
               "nzdep_decile", "arf_rate")

## 5 groups instead of 10
df$nzDepQuint <- cut(df$nzdep_decile, breaks = c(-Inf, 2, 4, 6, 8, Inf),
                     labels = c("1 to 2 (least deprived)",
                                "3 to 4",
                                "5 to 6",
                                "7 to 8",
                                "9 to 10 (most deprived)"))

df$log_population <- log(df$population)

fit <- glm(arf_count ~ Permethrin_rate +
             nzDepQuint +
             Pacific_proportion +
             Maori_proportion, offset=log_population,
           family = poisson(link = "log"),
           data = df)

fit1 <- glm(arf_count ~ Permethrin_rate +
             nzDepQuint +
             Pacific_proportion +
             Maori_proportion + offset(log(population)),
           family = quasipoisson(link = "log"),
           data = df)

fit5 <- glm(arf_count ~ Permethrin_rate +
             nzDepQuint +
             Pacific_proportion +
             Maori_proportion + offset(log(population)),
           family = quasipoisson(link = "log"),
           data = df)

mycounter <- 0
Qnum <- function(count = mycounter){
  mycounter <<- mycounter + 1
  paste0("Question ", mycounter)
}

knitr::opts_chunk$set(error = TRUE)
```


<center>
Scabies swimming in mineral oil at 40x magnification.

```{r echo = FALSE}
embed_url("https://www.youtube.com/watch?v=tR0p9s2Lzuk")
```

Live mites on human skin (courtesy of Berghofer QIMR).
```{r echo = FALSE}
embed_url("https://youtu.be/aarPJyjkW2w")
```

<!-- [![Live mites](http://img.youtube.com/vi/aarPJyjkW2w/0.jpg)](https://youtu.be/aarPJyjkW2w){target="_blank" width="600"} -->

</center>

## Getting started

This session, we will focus on modelling of count outcomes in
epidemiology.

We will learn how to:

-   Plot the relationships between count variables and other variables
-   Run and interpret regression models
-   Interpret the model fit and summary statistics of models
-   Check models for over-dispersion
-   Consider the effects of multi-collinearity on exposure-disease
    relationships
-   Use regression nomograms to better understand and communicate the results of regression models
  <br></br>
  
We will be leaning on the following libraries to do these analyses:

-   [`regplot`](https://cran.r-project.org/web/packages/regplot/index.html){target="_blank"}
-   [`PerformanceAnalytics`](https://cran.r-project.org/web/packages/PerformanceAnalytics/index.html){target="_blank"}
-   [`jtools`](https://jtools.jacob-long.com/){target="_blank"}
-   [`corrplot`](https://r-coder.com/correlation-plot-r/){target="_blank"}
-   [`finalfit`](https://finalfit.org/){target="_blank"} and
-   [`car`](https://socialsciences.mcmaster.ca/jfox/Books/Companion/index.html){target="_blank"}


## Background

We will be diving into a real-life research battle ground today.

<center>

![Research battle by Sander Meyer from
unsplash.com](./images/sander-meyer-kcwkT7mc0fU-unsplash.jpg "battle"){width="200"}

</center>

There is some controversy about the cause of acute rheumatic fever, 
which is overall rare disease, but it occurs much more commonly in 
Māori and Pacific children than children of any other ethnic group in NZ.

This disease often presents with fever and inflamed joints and can lead to
life-limiting sequelae which include thickened and dysfunctional heart valves.
If these are bad enough, it may lead to the need for cardiac surgery
and in some cases, premature death.

<center>

![Thickened heart valves associated with rheumatic heart
disease](images/arf.jfif){width="300"}

</center>

The most prevalent explanation in the health sector is that it is caused
by bacterial streptococcal throat infection.

An alternative explanation is that it is caused by the common skin
condition, *scabies*, which has the following skin appearance, which is
easy not to notice. The little crops of raised lumps are scabies and
they are caused by the microscopic mite pictured above.

<center>

![Typical skin appearance of scabies lesions: image courtesy of Dr
Daniel Engelman](images/Picture2.jpg){width="300"}

</center>

The treatment for scabies in New Zealand is pictured below, and we will
use records of pharmacy dispensing of this drug as an indirect measure of scabies prevalence in the community.

<center>

![Permethrin lotion](images/permethrin.jfif){width="300"}

</center>

The proposed mechanism by which scabies causes streptococcal infection
and rheumatic fever is given below:

[![Consequences of scabies infection. Lancet. 2019 Jul
6;394(10192):81-92. doi:
10.1016/S0140-6736(19)31136-5.](images/Rheumatic_fever_theory.png){width="700"}](https://linkinghub.elsevier.com/retrieve/pii/S0140-6736(19)31136-5)

[Consequences of scabies
infection](https://linkinghub.elsevier.com/retrieve/pii/S0140-6736(19)31136-5){target="_blank"}


We will be look for the evidence that prescribing for a certain treatment for scabies (the lotion **permethrin**) is linked with **acute rheumatic fever** at a **suburb level** (1 row of data for each suburb). Since the counts of acute rheumatic fever over a certain period constitute a 'count' variable, **Poisson** regression will be used. The population of the suburb will be used as an **offset** in the model. An [**offset**](https://stats.stackexchange.com/questions/175349/in-a-poisson-model-what-is-the-difference-between-using-time-as-a-covariate-or){target="_blank"} is often used as the denominator when rate values are modelled. For example, here, our outcome is the **count or number of permethrin scripts dispensed in a suburb** divided by the **population of that suburb**. The count of prescriptions is naturally influenced by the population, so this information is incorporated in the model as an offset. The offset allows us to model rates rather than counts. Technically, it is a covariate or independent variable whose value of beta coefficient is forced to be one.

Covariates, or potential confounders in the analysis include:

-   The proportion of Māori and Pacific people living in a suburb
-   Socioeconomic deprivation

We will use the counts of acute rheumatic fever by suburb (or census area unit). This information is derived from routine datasets used
in the health sector. The formal write-up of the analysis is
[here](https://onlinelibrary.wiley.com/doi/epdf/10.1111/jpc.14697){target="_blank"}.


#### Brief revision

Count regression is used when an outcome is an integer or count, rather than a continuous measure.

Examples of counts include:

- the number of hospital admissions per day
- the number of people who die each week
- the number of teeth in a person's mouth that affected by decay

In comparison, continuous measures can include fractions or decimals, such as:

- height
- weight
- blood pressure
- age.

Count variables have special characteristics of their distribution which makes linear regression unsuitable. First of all, the distribution is positive, asymmetric and right skewed, meaning a log transformation of the count is necessary. Also, the variance is usually proportional to the count, rather than a constant.

### Lecture revision

Our outcome is acute rheumatic fever counts and we will build models for this using regression. For the following 3 questions, please answer *True* of *False* in response to each statement.
  
##### **`r Qnum()`**
```{r revision_1, echo = FALSE}
question("The measure of association derived from a count model is an odds ratio.",
answer("True", message = "No, the odds ratio is the measure of association for logistic regression (next tutorial)"),
  answer("False", message = "Yes, the odds ratio is the measure of association for logistic regression (next tutorial). For count regression, the measure of association is the *incidence rate ratio*",  correct = TRUE),
  allow_retry = TRUE
)
```
  
##### **`r Qnum()`**
```{r revision_2, echo = FALSE}
question("Linear regression is unsuitable to use for count outcomes as it may predict negative counts",
  answer("True", message = "Yes, count regression overcomes this problem as the outcome is modelled on a log scale (outcome is log(count)). Exponentiating log(count) predictions gives count predictions that are always positive as exponentiating any non zero number (even negative) gives positive values only", correct = TRUE),
  answer("False", message = "No, linear regression may predict negative counts"),
  allow_retry = TRUE
)
```
  
##### **`r Qnum()`**
```{r revision_3, echo = FALSE}
question("Linear regression is unsuitable for modelling count outcomes since the variance is likely to increase with the mean and the errors will not be normally distributed",
  answer("True", message = "Yes. With count outcomes, the errors will not be normally distributed but linear regression assumes that they are. This happens when the variance increases with the mean. In contrast, count regression is suitable to use when the variance increases with the mean (e.g., when variance = mean)", correct = TRUE),
  answer("False", message = "No. With count outcomes, the errors will not be normally distributed but linear regression assumes that they are. This happens when the variance increases with the mean. In contrast, count regression is suitable to use when the variance increases with the mean (e.g., when variance = mean)"),
  allow_retry = TRUE
)
```


## **Set-up**

To set-up our session for analysis and call the necessary libraries, we’ll use `pacman::p_load()` instead of `install.packages()` and `library()`. The `pacman` library shortens this procedure, and
`p_load()` will look for a package and install it if it can't be found. It will also put it on the search path, like the `library()` command does. We won't run this code, as it will mess up our tutorial, but it is essential for getting this running on a local machine or posit cloud.

```{r library_1, eval=FALSE }

if(!require(pacman)) install.packages("pacman")
pacman::p_load(tidyverse, 
               epiDisplay,
               visdat,
               skimr,
               lattice,
               jtools,
               ggstance,
               broom.mixed,
               finalfit,
               dplyr,
               kableExtra,
               car,
               MixAll,
               regplot,
               PerformanceAnalytics,
               corrplot)
```
  
View the dataframe; called `for_shiny`. Notice that there’s 1 row for each suburb:

```{r view_1, exercise = TRUE}

for_shiny
```
  
List the variable names:
```{r names_1, exercise = TRUE}
for_shiny |> names()
```
  
Rename our dataframe as `df`. Then rename the variables so they're more understandable:
```{r rename_1, exercise = TRUE}
df <- for_shiny # Rename dataframe

# Rename variables
names(df) <- c("arf_count", "population", "Pacific_proportion",
               "Maori_proportion", "Permethrin_rate", "Suburb",
               "nzdep_decile", "arf_rate")
df |> names() # Check that the variables have been renamed
```
  
### **Data integrity checks**  

### 1) Duplicates
There should be 1 row for each `Suburb`, so check if this variable is duplicated.
```{r duplicate_1, exercise = TRUE}
stopifnot(df[df$Suburb |> duplicated(),] == 0)
```

  
If `Suburb` is duplicated, the R output would read: `df[df$Suburb |> duplicated(),] == 0 are not all TRUE`.  
Are there any duplicates?  

  
### 2) Ranges
```{r rename2, include=FALSE, eval = TRUE}
names(df) <- c("arf_count", "population", "Pacific_proportion",
               "Maori_proportion", "Permethrin_rate", "Suburb",
               "nzdep_decile", "arf_rate")
```

```{r range_1, exercise = TRUE}
df |> skim()
```
Are there any implausible extreme values (check `p0` [minimum] and `p100` [maximum])?
  
##### **`r Qnum()`**
From the summary data above, what is the *standard error* of `arf_count`?

Hint, you will need to use the square root `sqrt()` function and use R as a calculator.

```{r variance_1, exercise = TRUE}
```
```{r variance_1-solution}
1.17/sqrt(413)
```
::: {#variance_1-hint}
**Hint:** standard error = standard deviation/square root(number of observations)
:::
```{r variance_1-check}
grade_this( {
if(isTRUE(all.equal(.solution, .result, tolerance = 0.1) )){
    pass("Yes, standard error = 1.17/ sqrt(413) = 0.0576")
  }
  fail("Please try again")
})
```

##### **`r Qnum()`**
From the summary data above, what is the *variance* of `arf_count`?
```{r variance_2, exercise = TRUE}
```
```{r variance_2-solution}
(1.17/sqrt(413))^2
```
::: {#variance_2-hint}
**Hint:** Variance = (standard error)^2
:::
```{r variance_2-check}
grade_this( {
if(isTRUE(all.equal(.solution, .result, tolerance = 0.1) )){
    pass("Yes, variance = (standard error)^2")
  }
  fail("Please try again")
})
```
  
### 3) Missing data
```{r missing_1, exercise = TRUE}
df |> vis_miss(sort_miss = TRUE)
```
Do you see any missing data?

## **Univariable distributions: histograms**
Use the `histogram()` function for these.  
  
Outcome as a *count*: `arf_count`
```{r histogram_1, exercise = TRUE}
df$arf_count |> histogram()
```
  
Outcome as a *rate*: `arf_rate`
```{r histogram_2, exercise = TRUE}
df$arf_rate |> histogram()
```

  
##### **`r Qnum()`**
```{r histogram_3, echo = FALSE}
question("To decide what type of regression to use for our models, we need to check for overdispersion.  
Look at your first histogram above – for `arf_count`. In the previous section, look at the mean of `arf_count` displayed from using the `skim` function and compare it with the variance you calculated. Which of the following, if any, is consistent with overdispersion?",
  answer("Variance < mean", message = "Yes. The variance (0.0033) is substantially smaller than the mean (0.55).", correct = TRUE),
  answer("Long tail of the distribution only", message = "No. A long tail is a feature of a power distribution. Please try again"),
  allow_retry = TRUE
)
```
 
  
##### **`r Qnum()`**
```{r histogram_4, echo = FALSE}
question("Your results should point to evidence of under dispersion. Given this observation, what type of regression would improve model fit?",
  answer("Linear", message = "The outcome, ARF count, comprises whole numbers, even if there is overdispersion. So linear regression is unsuitable. Please try again"),
  answer("Poisson", message = "No, as there is overdispersion, we need to account for this by modelling a variant of Poisson regression. Please try again"),
  answer("Quasi-Poisson regression or negative-binomial", message = "Yes, these forms of regression are used for accounting for overor under dispersion when modelling outcomes comprising counts", correct = TRUE),
  answer("Logistic regression", message = "This is used when the outcome is binary (2 values only), but our outcome (ARF count) comprises positive integers with more than 2 values. Please try again"),
  allow_retry = TRUE
)
```

First, we will check the distribution of our outcome
  
Exposure: `permethrin_rate`
```{r histogram_5, exercise = TRUE}
df$Permethrin_rate |> histogram()
```

##### **`r Qnum()`**  
Then the distribution of our confounders. Adjust the following code to plot three histograms of the following variables.
```{r histogram_6, exercise = TRUE}
df$Pacific_proportion 
df$Maori_proportion
df$nzdep_decile 
```
```{r histogram_6-solution}
df$Pacific_proportion |> histogram()
df$Maori_proportion |> histogram()
df$nzdep_decile |> histogram()
```
  
::: {#histogram_6-hint}
**Hint:** You need to use the `histogram()` function
:::


```{r histogram_6-check}
grade_this_code("Good, you can now visualise the distribution of continuous and count variables with histograms!")
```
  
##### **`r Qnum()`**  
```{r histogram_7, echo = FALSE}
question("How would you interpret the histogram for `Pacific_proportion`?",
  answer("The proportion of Pacific people is evenly distributed in suburbs in Auckland", message = "No, in fact, you can see that it is very uneven! Please try again"),
  answer("Most suburbs in Auckland have a high proportion of Pacific people living in them", message = "No, please reconsider the plot and try again. Please try again"),
  answer("A small number of suburbs in Auckland have a very high proportion of Pacific people living in them", message = "Yes, this is what the histogram shows.", correct = TRUE),
  allow_retry = TRUE
)
```

When a categorical variable has many groups, it’s common to combine these to avoid sparse data. For `nzdep_decile`, which has 10 categories (last histogram above), we’ll combine consecutive pairs to create quintiles (5 groups). The breaks are included in the category. So, for example, the first category includes any value from negative infinity (`-Inf`) to 2. Any value greater than two will be incorporated in the next category.
```{r histogram_8, exercise = TRUE}
# 5 groups instead of 10
df$nzDepQuint <- cut(df$nzdep_decile, breaks = c(-Inf, 2, 4, 6, 8, Inf),
                     labels = c("1 to 2 (least deprived)",
                                "3 to 4",
                                "5 to 6",
                                "7 to 8",
                                "9 to 10 (most deprived)"))

# Check the frequency of our 5 groups
epiDisplay::tabpct(df$nzDepQuint, df$nzdep_decile)
```

## **Bivariate associations: scatterplots**
Use the `car::scatterplot()` function for this.
  
####Exposure and outcome
```{r scatterplot_1, exercise = TRUE}
car::scatterplot(arf_rate ~ Permethrin_rate, data = df)
```
How would you describe the association between the exposure and outcome?

##### **`r Qnum()`**  
```{r scatter_1, echo = FALSE}
question("How would you interpret the scatter plot between `permethrin_rate` dispensing and `arf_rate`?",
  answer("No evidence of association", message = "No, this would be expected if there were a horizontal regression line"),
  answer("Negative association", message = "No, we would expect to see a line in the top right hand corner going to the bottom left."),
  answer("A positive association", correct = TRUE, message = "Yes, the greater the permethrin dispensing, the greater the rate of rheumatic fever"),
  allow_retry = TRUE
)
```

##### **`r Qnum()`**  
Complete the following lines of code to make scatter plots between covariates and the outcome `arf_rate`.  
Outcome vs. potential covariates:
```{r scatterplot_2, exercise = TRUE}
arf_rate ~ nzdep_decile
arf_rate ~ Pacific_proportion 
arf_rate ~ Maori_proportion 
```

::: {#scatterplot_2-hint}
**Hint:** You need to use the `car::scatterplot()` function. You'll also need to fill the `data = ...` argument, so that `R` knows which variables in which `data.frame` you are referring to.
:::

```{r scatterplot_2-solution}
car::scatterplot(arf_rate ~ nzdep_decile, data = df)
car::scatterplot(arf_rate ~ Pacific_proportion, data = df)
car::scatterplot(arf_rate ~ Maori_proportion, data = df)
```


```{r scatterplot_2-check}
grade_this_code("Good, you can now visualise the distribution of two continuous variables!")
```

Are the potential confounders associated with the outcome?
  
##### **`r Qnum()`**  
```{r scatter_2_int, echo = FALSE}
question("How would you interpret the scatter plot between the potential confounders and `arf_rate`?",
  answer("No evidence of association in any variables", message = "No, this would be expected if the regression lines were horizontal"),
  answer("All Negative associations", message = "No, we would expect to see lines in the top right hand corner going to the bottom left."),
  answer("All positive associations", correct = TRUE, message = "Yes, the greater the permethrin dispensing, the greater the rate of rheumatic fever"),
  answer("Something else", correct = TRUE, message = "No, please try again!"),
  allow_retry = TRUE
)
```



Re-plot these scatter plots using the `ylab` function to label the y-axis:
```{r scatterplot_3, exercise = TRUE}
car::scatterplot(arf_rate ~ Pacific_proportion, data = df,
                  ylab = "Permethrin rate per 100,000 per year")

car::scatterplot(Permethrin_rate ~ Maori_proportion, data = df,
                  ylab = "Permethrin rate per 100,000 per year")

car::scatterplot(Permethrin_rate ~ nzdep_decile, data = df,
                  ylab = "Permethrin rate per 100,000 per year")
```
  
**Quick method to make several scatterplots**  
Use the `PerformanceAnalytics::chart.Correlation()` function to get all correlations in 1 chart. A short-cut!
Here, we select 5 variables from `df` and put them into a dataframe called `my_data`. The we apply our `PerformanceAnalytics::chart.Correlation` function to `my_data` to get histograms, scatter plots and Pearson correlation coefficients.
```{r scatterplot_4, exercise = TRUE}
## Select variables of interest into new data.frame
my_data <- df[, c("Pacific_proportion", "Maori_proportion", "Permethrin_rate", "nzdep_decile", "arf_rate")]

## Chart the correlations and regressions and distributions of each variable.
PerformanceAnalytics::chart.Correlation(my_data, histogram = TRUE, pch = 19)
```
  
##### **`r Qnum()`**
```{r scatterplot_5, echo = FALSE}
question("What is the Pearson correlation coefficient (`r`) for the association between Pacific proportion and permethrin rate?",
  answer("`r` = 0.55", message = "No, `r` = 0.55 for the correlation between `Pacific_proportion` and `Maori_proportion`"),
  answer("`r` = 0.66", message = "No, `r` = 0.66 for the correlations of `Permethrin_rate` with `Maori_proportion` and `arf_rate`"),
  answer("`r` = 0.88", message = "Yes, the row for `Pacific_proportion` intersects with the column for `Permethrin_rate` at the cell that has 0.88 in it. A correlation of 0.88 is extremely high and suggests a very strong association.", correct = TRUE),
  answer("`r` = 0.79", message = "No, `r` = 0.79 for the correlation between `Permethrin_rate` and `nzdep_decile`"),
  allow_retry = TRUE
)
```

## **Regression**
We will build a Poisson regression model with our outcome as `arf_count` and exposure as `Permethrin_rate`. To adjust for population size, we will include `log_population` as an offset. We will also adjust for NZDep quintile (`nzDepQuint`), `Pacific_proportion` and `Maori_proportion`.

Run the following code to make a Poisson regression model of `arf_count` with all other risk factors as independent variables or covariates.

```{r regression_1, exercise = TRUE}
## Make offset variable
df$log_population <- log(df$population)

## Build model
fit <- glm(arf_count ~ Permethrin_rate +
             nzDepQuint +
             Pacific_proportion +
             Maori_proportion, offset = log_population,
           family = poisson(link = "log"),
           data = df)

## Print summary of model
summary(fit)
```
  
Look at the term, `offset = log_population`, in the model. With counts (`arf_count`) as the outcome, having `log_population` as the offset makes the model represent a *rate* (count per unit *population*).  

Check your understanding.

##### **`r Qnum()`**
```{r regression_2, echo = FALSE}
question("Commonly, offsets are based on the time spent observing each outcome (observation period). When would it be appropriate to include `log(time in years)` as an offset? (select all that apply)",
  answer("If the observation period differed by suburb", message = "First, if the observation period differed by suburb, then  larger numbers of cases would be likely to in suburbs observed for longer periods. Having an offset based on time accounts for this.", correct = TRUE),
  answer("If we wanted our outcome to be rates based on counts per *year*", message = "Second, with counts (`arf_count`) as the outcome, adding `log(time)` as the offset makes the model represent a rate in counts per *year*.", correct = TRUE),
  answer("If we had data on the observation period of each suburb", message = "Third, we need the observation period for each suburb.", correct = TRUE),
  answer("None of the above", message = "Please try again"),
  allow_retry = TRUE
)
```
  
##### **`r Qnum()`**
```{r regression_3, echo = FALSE}
question("For `Permethrin_rate`, you should get an `Estimate` value of 3.527e-04. What does this tell us?",
  answer("There is, on average, 3.527e-04 = 0.0003527 more acute rheumatic fever for every 1-unit increase in permethrin rate", message = "No. 3.527e-04 is measured on a natural-log scale. So you need to exponentiate it to give a meaningful interpretation"),
  answer("The incidence rate of acute rheumatic fever is, on average, 3.527e-04 = 0.0003527 times higher for every 1-unit increase in permethrin rate", message = "No. 3.527e-04 is measured on a natural-log scale. So you need to exponentiate it to interpret the figure."),
  answer("There is, on average, exp(3.527e-04) more acute rheumatic fever for every 1-unit increase in permethrin rate", message = "No. Exponentiating this converts it into an incidence rate ratio. However, *exp(3.527e-04) more* does not imply a ratio. What word do we need to add to imply a ratio?"),
  answer("There is, on average, exp(3.527e-04) = 1.00035 times more acute rheumatic fever for every 1-unit increase in permethrin dispensing rate", message = "Yes, exponentiating 3.527e-04 converts it into an incidence rate ratio. The word *times* suggests a ratio", correct = TRUE),
  allow_retry = TRUE
)
```

While the exponentiated coefficient don't seem large (1.00035 is a 0.035% change, with a one unit increase of permethrin), the nature of the association may only be understood by also knowing to what extent permethrin dispensing varies in the population. From our `skim()` function used earlier, we can see that the minimum values of permethrin dispensing is 0 and the maximum is 3,201. Therefore, the ratio of risk for a change of 3,000 is `exp(3.527e-04 * 3000)` which equals 2.88. This means that comparing the lowest to highest suburbs results in a 100 \* (2.88 - 1) = 188% change in acute rheumatic fever incidence. That weak association is now looking *very strong*. 
  
Exponentiate our model coefficients with the `jtools::sum` function:
```{r regression_4, exercise = TRUE}
jtools::summ(fit, exp = TRUE)
```
  
##### **`r Qnum()`**
```{r regression_5, echo = FALSE}
question("For `Pacific_proportion`, you get an `exp(Est.)` value of 11.56. What does this tell us?",
  answer("There is, on average, 11.56 more acute rheumatic fever for every 1-unit increase in Pacific proportion", message = "No. 11.56 is a rate. What word do we need to add to imply a ratio?"),
  answer("The incidence of acute rheumatic fever is, on average, 11.56 times higher for every 1-unit increase in Pacific proportion", message = "Well done! *11.56 times* implies a ratio and 11.56 is the incidence rate ratio", correct = TRUE),
  answer("There is, on average, `exp(11.56)` more acute rheumatic fever for every 1-unit increase in Pacific proportion", message = "No, 11.56 is already an exponentiated value (R code above reads `exp = TRUE`) – you don’t need to exponentiate again!"),
  answer("The incidence rate of acute rheumatic fever is, on average, `exp(11.56)` times higher for every 1-unit increase in Pacific proportion", message = "No, 11.56 is already an exponentiated value (R code above reads `exp = TRUE`) – you don’t need to exponentiate again!"),
  allow_retry = TRUE
)
```
  
##### **`r Qnum()`**
We saw before that, for the outcome, there’s evidence of underdispersion. To account for this, we will construct a *quasiPoisson* model (please use `fit_qp` as the new model object. Do this by changing `poisson` to `quasipoisson` in the code below and running it.
```{r regression_6, exercise = TRUE}
fit1 <- glm(arf_count ~ I(Permethrin_rate/1000) +
             nzDepQuint +
             Pacific_proportion +
             Maori_proportion + offset(log(population)),
           family = poisson(link = "log"),
           data = df)
summary(fit1)
```

```{r regression_6-solution}
fit_qp <- glm(arf_count ~ I(Permethrin_rate/1000) +
             nzDepQuint +
             Pacific_proportion +
             Maori_proportion + offset(log(population)),
           family = quasipoisson(link = "log"),
           data = df)
summary(fit_qp)
```

::: {#regression_6-hint}
**Hint:** Please change the `family` argument, and change the name of the model object from `fit1` to `fit_qp`.
:::

```{r regression_6-check}
grade_this_code("Good. It’s important to know how to change to a `quasipoisson` model when there is evidence of over or under dispersion")
```
  
##### **`r Qnum()`**
Using the code format below, exponentiate your $\beta$ coefficients so that you get incidence rate ratios.
```{r regression_7, exercise = TRUE}
# jtools::summ(model_object, exp = logical_value)
```

```{r regression_7-solution}
jtools::summ(fit_qp, exp = TRUE)
```

::: {#regression_7-hint}
**Hint:** What is the new model object names and `logical_value` for this question?
:::

```{r regression_7-check}
grade_this_code("Great! Above, we created a model named `fit_qp`. As we want to exponentiate, the logical value for `exp` will be `TRUE`")
```
  
Now, use the `jtools::plot_summs()` function to plot the rate ratios:
```{r regression_8, exercise = TRUE}
jtools::plot_summs(fit_qp, exp = TRUE)
```

You should get a graph showing rate ratios:  

-  Circles for the `Estimates` 
-  Horizontal lines for the 95% confidence intervals
-  A dotted vertical line for a ratio of 1: the null value 
  
Which variables are significantly associated with the outcome?

## **Multi-collinearity**

In a model, when 2 or more predictors are *strongly correlated* with one other, this is called **multicollinearity**. In a model, it’s a problem as it:  

-   increases the variance of estimates of regression coefficients
-   makes these estimates unstable and unreliable  <br>

We can detect it by looking for strong correlations between variables.
  
Use the *cor()* function to get the correlation between `Pacific_proportion` and `Permethrin_rate`:

```{r multicollinearity_1, exercise = TRUE}
cor(df$Pacific_proportion, df$Permethrin_rate)
```

You should get a correlation of 0.88, which is **strong**. Thus, multicollinearity may be a problem for our analysis.  
To deal with this, we can remove one of these predictors from the model. So that we retain our exposure (`Permethrin_rate`) in our model, let’s remove `Pacific_proportion` by adding `#` before it.  
Another change is that we'll divide `Permethrin_rate` by 1000. This will increase the coefficient for this variable by 1000 so that it’s more meaningful. (This is like converting grams into kilograms; the latter is more meaningful for larger weights). Below, we use the `I()` function to do this calculation within the 'model' argument.  
Now run our revised code below:

```{r regression_9, exercise = TRUE}
df$log_population <- log(df$population)

fit3 <- glm(arf_count ~ I(Permethrin_rate/1000) +
             nzDepQuint +
             #Pacific_proportion +
             Maori_proportion, offset=(log(population)),
           family = quasipoisson(link = "log"),
           data = df)

summary(fit3)
```

Was is the slope for `Permethrin_rate/1000`?
Is it statistically significant?  
  
Run the code for the next model, which includes `Permethrin` as a predictor instead of `Permethrin_rate/1000.`
```{r regression_10, exercise = TRUE}
df$log_population <- log(df$population)

fit4 <- glm(arf_count ~  I(Permethrin_rate/1000) +
             nzDepQuint +
             #Pacific_proportion +
             Maori_proportion, offset=log_population,
           family = quasipoisson(link = "log"),
           data = df)

summary(fit4)
```

Was is the incidence rate ratio for `I(Permethrin_rate/1000)`?

This next model has `Pacific_proportion` included again. So we can test the effect of multi-collinearity. Run the code below.

```{r regression_11, exercise = TRUE}
fit5 <- glm(arf_count ~ I(Permethrin_rate/1000) +
             nzDepQuint +
             Pacific_proportion +
             Maori_proportion, offset(log(population)),
           family = quasipoisson(link = "log"),
           data = df)

summary(fit5)
```

Because of multicollinearity, we expect the standard error for `I(Permethrin_rate/1000)` to *increase* after `Pacific_proportion` has been added. Has it?


## **Regression nomogram**

This is a visual summary of structure of a model.
We will display regression nomograms by suburb. First, let’s print the names of the `Suburb`s in our dataset to see if there is one we recognise.
```{r nomogram_1, exercise = TRUE}
df$Suburb |> print()
```

The code below will run in posit, but unfortunately not on this platform, so we will just show you the code and the output. We have plotted the values for Otara East, highlighted on the figure in [red]{style="color: red;"}.

```{r nomogram_2,  warning = FALSE, message = FALSE, fig.show = TRUE}

regplot::regplot(fit, 
                 observation = df[df$Suburb == "Otara East",], clickable = FALSE)

```

![Regplot](./images/regplot.png "Regression illustrated")

For Otara East, for example, we see that `Pacific_proportion` is about 0.8 or 80%.
Also, the dispensing rate of permethrin `Permethrin_rate` is relatively high, as expected by the correlation coefficient between this variable and `Pacific_proportion` as we discussed earlier. Each variable contributes a score (`β(X-m) terms`) and these add up as a type of weighted average to produce a predicted rate of acute rheumatic fever (`arf_count`). Note that for Otara East, the predicted `arf_count` is very high.

  
Please attempt the next 2 questions.  
 
  
##### **`r Qnum()`**
```{r nomogram_3, echo = FALSE}
question("Which term (predictor) in the model contributes *most* to the outcome for Otara East?",
  answer("`log_population`", message = "No, if you identify the red dot in the graph for `log_population` and read vertically up from it, you’ll touch the 2nd red dot (4th furthest) from 0 in the top line (`β(X-m) terms`). So `log_population` will contribute to the *4th most* to the outcome. Please try again"),
  answer("`Maori_proportion`", message = "No, if you identify the red dot in the graph for `Maori_proportion` and read vertically up from it, you’ll touch the red dot that’s closest from 0 in the top line (`β(X-m) terms`). So `Maori_proportion` will contribute the *least* to the outcome. Please try again"),
  answer("`Pacific_proportion`", message = "Yes, if you identify the red dot that’s furthest from 0 in the top line – β(X-m) terms – then read vertically down from it, you’ll touch the red dot in the graph for `Pacific_proportion`", correct = TRUE),
  answer("`nzDepQuint`", message = "No, if you identify the red dot in the graph for `nzDepQuint` and read vertically up from it, you’ll touch the 4th red dot (2nd furthest) from 0 in the top line (`β(X-m) terms`). So `nzDepQuint` will contribute the *2nd most* to the outcome. Please try again"),
  answer("`Permethrin_rate`", message = "No, if you identify the red dot in the graph for `Permethrin_rate` and read vertically up from it, you’ll touch the 3^rd^ red dot (3^rd^ furthest) from 0 in the top line (`β(X-m) terms`). So `Permethrin_rate` will contribute the *3rd most* to the outcome. Please try again"),
  allow_retry = TRUE
)
```
(Hint: In the top line, `β(X-m) terms`, identify the red dot that’s furthest from 0. Then read vertically down from it)  
  
##### **`r Qnum()`**
```{r nomogram_4, echo = FALSE}
question("Which of the following is much higher for Otara East compared to the suburb averages?",
  answer("`Total Score` and all predictors", message = "No, this is not true for all predictors: the central position of the red dot in the plot for `log_population` indicates that the population for Otara East is similar to the median population (for all suburbs). Please try again"),
  answer("`Total Score` and all predictors except `log_population`", message = "Yes, for these terms, the red dot in each plot is to the far right of the distribution for all suburbs. The high `Total Score` for Otara East (4.22), which equates to an ARF count of 5.52, is due to high average values for all predictors except `log_population`", correct = TRUE),
  answer("All predictors except `log_population` but not `Total Score`", message = "No, the red dot in the `Total score` plot is to the far right of the distribution for all suburbs. Please try again"),
  answer("`Total Score` only", message = "No, the red dot is, not only to the far right of the plot for `Total Score`, but also for most predictors. Please try again"),
  allow_retry = TRUE
)
```
Hint: In each density plot or histogram, look at where the red dot (for Otara East) is.
  


```{r nomogram_5, warning = FALSE, message = FALSE, fig.show = TRUE}
regplot::regplot(fit, observation = df[df$Suburb == "Remuera South",])
```
![Regplot2](./images/remuera_regplot.png "Regression illustrated")

## **Model fit**
How well a model fits the data can be assessed with a 'marginal model plot', which shows the relationship between model predicted (modelled) and observed data (observations). In a good fitting model, we expect to see the two lines almost superimposed on each other. Regions of poor fit may be identified by seeing separation between the two lines.

Run this code to produce a marginal model plot for our `fit` model object:
```{r model_fit_1, exercise = TRUE}
car::marginalModelPlots(fit)
```

What is the nature of the overall fit of the data to the model?  
-  **Good fit** is indicated by **good alignment** between `Data` (blue line) and the `Model` (red line)  
-  **Poor fit** is indicated by **poor alignment** between these lines  
-   Often, for more extreme or outlying values of an exposure, there is deviation of the `Data` from the `Model`. This may not invalidate your model, but would be important to be aware of, and may be worthwhile noting this in your results.
  
##### **`r Qnum()`**
```{r model_fit_2, echo = FALSE}
question("Which of the following terms for the model would be a POOR fit for the data?",
  answer("`Permethrin_rate`", message = "No, that’s harsh! There’s only poor alignment between the *data* (blue) and *model* (red) curves for `Permethrin_rate`> 2500 but this is minor as there’s few observations in that extreme region. Please try again"),
  answer("`Permethrin_rate` and `Pacific_proportion`", message = "No, that’s harsh! For `Permethrin_rate`, there’s only poor alignment between the *data* (blue) and *model* (red) curves for Permethrin_rate>2500 but this is minor as there’s few observations in that extreme region. For `Pacific_proportion`, the *data* (blue) and *model* (red) curves are misaligned in some areas of the plot but the degree of deviation is minor. Please try again"),
  answer("None of these", message = "Yes, the *data* (blue) and *model* (red) curves are well-aligned for `Maori_proportion` and the overall model (`Linear_Predictor`). And although there is some misalignment in the other 2 plots, this is minor", correct = TRUE),
  allow_retry = TRUE
)
```

## Publication-quality regression tables
In published articles, it’s common to present univariable (unadjusted) and multivariable (adjusted) regression coefficients in a table. To create these, run the following code.

```{r publication_1, exercise = TRUE}
df$Permethrin_rate_per_1000 <- df$Permethrin_rate/1000  # Like converting grams into kilograms
df$log_population <- log(df$population)  # Offset

dependent <- "arf_count"
explanatory <- c("Pacific_proportion", "Maori_proportion", 
                 "Permethrin_rate_per_1000","nzdep_decile")

# `fit_uni` creates univariable (unadjusted) models
fit_uni = df |> 
  glmuni(dependent, explanatory, family = quasipoisson,
         offset = df$log_population) |> 
  fit2df(estimate_name = "Rate ratio (univariable)")

# `fit_multi` creates a multivariable (adjusted) model
fit_multi = df |> 
  glmmulti(dependent, explanatory, family = quasipoisson, 
           offset = df$log_population) |> 
  fit2df(estimate_name = "Rate ratio (multivariable)")

df |>
  ## Crosstable
  summary_factorlist(dependent, explanatory, cont = "median", fit_id=TRUE)  |> 
  
  #### Add univariable table
  ff_merge(fit_uni, estimate_name = "Rate ratio") |> 
  
  #### Add multivariable table
  ff_merge(fit_multi, estimate_name = "Rate ratio",
           last_merge = TRUE) |> 
dependent_label(df, dependent) -> regression_table

regress_tab <- knitr::kable(regression_table, row.names = FALSE, 
                            align = c("l", "l", "r", "r", "r"))

#### Puts output into viewer so that it can be cut and pasted into Excel, 
#### then Word.
regress_tab |> kableExtra::kable_styling()
```

Notice that the adjusted `Rate ratio (multivariable)` for `Permethrin_rate_per_1000`
 is not statistically significant. Does that mean permethrin or scabies is not related to acute rheumatic fever incidence. 

```{r pub_model, echo = FALSE}
question("What is the explanation for no significant association with `Permethrin_rate_per_1000` and `arf_count` in the multivariable model, but an association in the crude model?",
  answer("Multicollinearity", message = "Yes, if we leave out the collinear variable `Pacific_proportion`, we find an adjusted rate ratio between `Permethrin_rate_per_1000` and the outcome is significant.", correct = TRUE),
  answer("There really is no relationship between the two variables.", message = "No, think about the issue of the strong association between `Pacific_proportion` and `Permethrin_rate_per_1000`."),
  answer("Correcting for confounding factors has resulted in no association.", message = "This is a possible explanation, and if we believed Pacific ethnicity to be a shared common cause of permethrin and rheumatic fever, it could be possible. In actual fact, given the very strong correlation between permethrin and Pacific ethnicity makes multi-collinearity more likely."),
  allow_retry = TRUE
)
```


## Homework
Select only suburbs that have a socioeconomic deprivation of 6 or more:  
df_dep <- df[df$nzdep_decile >= 6, ]  

Then, for this subgroup, repeat the analyses we’ve covered in this tutorial.  

Please have a go at these tasks in posit.